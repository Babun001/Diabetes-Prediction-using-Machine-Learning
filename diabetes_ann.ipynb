{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>763</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>764</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>765</td>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>766</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  \\\n",
       "0             0            6      148             72             35        0   \n",
       "1             1            1       85             66             29        0   \n",
       "2             2            8      183             64              0        0   \n",
       "3             3            1       89             66             23       94   \n",
       "4             5            5      116             74              0        0   \n",
       "..          ...          ...      ...            ...            ...      ...   \n",
       "634         763           10      101             76             48      180   \n",
       "635         764            2      122             70             27        0   \n",
       "636         765            5      121             72             23      112   \n",
       "637         766            1      126             60              0        0   \n",
       "638         767            1       93             70             31        0   \n",
       "\n",
       "      BMI  DiabetesPedigreeFunction  Age  Outcome  \n",
       "0    33.6                     0.627   50        1  \n",
       "1    26.6                     0.351   31        0  \n",
       "2    23.3                     0.672   32        1  \n",
       "3    28.1                     0.167   21        0  \n",
       "4    25.6                     0.201   30        0  \n",
       "..    ...                       ...  ...      ...  \n",
       "634  32.9                     0.171   63        0  \n",
       "635  36.8                     0.340   27        0  \n",
       "636  26.2                     0.245   30        0  \n",
       "637  30.1                     0.349   47        1  \n",
       "638  30.4                     0.315   23        0  \n",
       "\n",
       "[639 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ FROM CSV AND CREATE A DATA FRAME\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Diabetes dataset/Clean_diabetes_dataset.csv')\n",
    "df\n",
    "\n",
    "# df_male = df.loc[df['gender'] == 'Male']    # GET ONLY THE MALE DATA(REASON: APATOTO FEMALE DATA THAK)\n",
    "\n",
    "# df = df.drop(\"gender\",axis=\"columns\")     # REMOVE GENDER COLUMN\n",
    "\n",
    "# df['gender'].replace('Male', 1, inplace=True)\n",
    "# df['gender'].replace('Female', 0, inplace=True)\n",
    "# df = df.loc[df['gender'] != 'Other']\n",
    "\n",
    "# df = df.drop(\"smoking_history\",axis=\"columns\")     # REMOVE SMOKING HISTORY COLUMN(REASON: DONT KNOW HOW SMOKING AFFECTS DIABETES)\n",
    "# df_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']\n",
    "# for i in col:\n",
    "#   df[i].replace(0,df[i].mean(),inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    439\n",
       "1    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT DF_MALE INTO X & Y\n",
    "# X = INDEPENDENT VARIABLES\n",
    "# Y = EPENDENT VARIABLES\n",
    "y = df['Outcome'].copy()\n",
    "X = df.drop(['Outcome','DiabetesPedigreeFunction'], axis=\"columns\")\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLE ADTA IMBALANCE - INCREASE/DECREASE ROWS WITH CLASS VALUES(0/1) IF ONE IS LOWER IN NUMBER\n",
    "# FOR BETTER TRAINING\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X_balanced, y_balanced = SMOTE().fit_resample(X, y)\n",
    "# y_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Scale X with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 32)                288       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2929 (11.44 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Dense(32, input_shape=(8,), activation='relu'))\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# model.add(keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=8, activation= 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary(\n",
    "    expand_nested=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 2s 4ms/step - loss: 0.5892 - accuracy: 0.6810\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4985 - accuracy: 0.7339\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.7808\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7769\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4311 - accuracy: 0.7906\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.7906\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8102\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4081 - accuracy: 0.8063\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8102\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8023\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4002 - accuracy: 0.8102\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3962 - accuracy: 0.8023\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8141\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8141\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3831 - accuracy: 0.8219\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.8239\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8317\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3753 - accuracy: 0.8317\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3641 - accuracy: 0.8258\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3594 - accuracy: 0.8317\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3559 - accuracy: 0.8356\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3552 - accuracy: 0.8297\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8434\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8395\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3363 - accuracy: 0.8474\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3340 - accuracy: 0.8513\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3259 - accuracy: 0.8513\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8513\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3169 - accuracy: 0.8611\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3180 - accuracy: 0.8571\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.8552\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8630\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8689\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.8748\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2916 - accuracy: 0.8826\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2777 - accuracy: 0.8826\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.8924\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.8982\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2761 - accuracy: 0.8708\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2637 - accuracy: 0.8943\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.8924\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2445 - accuracy: 0.9119\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2346 - accuracy: 0.9119\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2269 - accuracy: 0.9100\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.9178\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9139\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2046 - accuracy: 0.9276\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2055 - accuracy: 0.9217\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1960 - accuracy: 0.9256\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1938 - accuracy: 0.9335\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1790 - accuracy: 0.9413\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1757 - accuracy: 0.9413\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1724 - accuracy: 0.9315\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1661 - accuracy: 0.9432\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1578 - accuracy: 0.9432\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1640 - accuracy: 0.9472\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1523 - accuracy: 0.9452\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1511 - accuracy: 0.9491\n",
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.9432\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1369 - accuracy: 0.9569\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9550\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9569\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1217 - accuracy: 0.9648\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1254 - accuracy: 0.9609\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1122 - accuracy: 0.9589\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1080 - accuracy: 0.9667\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1035 - accuracy: 0.9726\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.9706\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9491\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9628\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.9746\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1292 - accuracy: 0.9609\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9726\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0824 - accuracy: 0.9804\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0966 - accuracy: 0.9765\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9785\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0736 - accuracy: 0.9863\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 0.9883\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9863\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0601 - accuracy: 0.9922\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9883\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0559 - accuracy: 0.9902\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9922\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9922\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9961\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9902\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0449 - accuracy: 0.9980\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.9804\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9941\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9980\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0405 - accuracy: 0.9922\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9961\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9980\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9941\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.9980\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.9941\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9883\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9726\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9335\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9785\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.9941\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=10,\n",
    "#     # callbacks=[earlystop_loss],\n",
    "# )\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 2.0267 - accuracy: 0.7656\n",
      "Accuracy: 76.56\n",
      "2.026717185974121\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# print(\"Epochs run:\", len(history.history[\"loss\"]))\n",
    "\n",
    "# print(history.history.keys())\n",
    "# acc = history.history[\"accuracy\"]\n",
    "# loss = history.history[\"loss\"]\n",
    "\n",
    "# val_acc = history.history[\"val_accuracy\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# # Train and validation accuracy\n",
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.ylim((0, 1))\n",
    "# plt.plot(epochs, acc, label=\"Training accurarcy\")\n",
    "# plt.plot(epochs, val_acc, label=\"Validation accurarcy\")\n",
    "# plt.title(\"Training and Validation accurarcy\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Train and validation loss\n",
    "# plt.subplot(1, 2, 2)\n",
    "\n",
    "# plt.plot(epochs, loss, label=\"Training loss\")\n",
    "# plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
    "# plt.title(\"Training and Validation loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER TUNING\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# # defining parameter range\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf']} \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# # fitting the model for grid search\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # print best parameter after tuning\n",
    "# print(grid.best_params_)\n",
    "  \n",
    "# # print how our model looks after hyper-parameter tuning\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# PREDICT RESULTS\n",
    "\n",
    "# make probability predictions with the model\n",
    "y_pred = model.predict(X_test)\n",
    "# round predictions \n",
    "y_pred_rounded = [round(x[0]) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONFUSION MATRIX\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "cm = confusion_matrix(y_test, y_pred_rounded)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:2539\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   2405\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2406\u001b[0m     {\n\u001b[0;32m   2407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2430\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2431\u001b[0m ):\n\u001b[0;32m   2432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2433\u001b[0m \n\u001b[0;32m   2434\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2536\u001b[0m \u001b[38;5;124;03m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2537\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2539\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2542\u001b[0m         labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://6b71e845-940e-40e7-9626-17c78c505ffe/assets\n"
     ]
    }
   ],
   "source": [
    "# SAVING THE MODEL USING PICKLE PACKAGE\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"diabetes-model-ann.pkl\"  \n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOAD AND USE THE SAVED MODEL USING PICKLE PACKAGE\n",
    "# with open(model_pkl_file, 'rb') as file:  \n",
    "#     model = pickle.load(file)\n",
    "\n",
    "# # evaluate model \n",
    "# y_predict = model.predict(X_test)\n",
    "\n",
    "# # check results\n",
    "# pred = model.evaluate(X_test, y_test)\n",
    "# print(f\"Accuracy : {pred * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
