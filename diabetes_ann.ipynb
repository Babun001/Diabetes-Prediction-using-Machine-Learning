{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>763</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>764</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>765</td>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>766</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  \\\n",
       "0             0            6      148             72             35        0   \n",
       "1             1            1       85             66             29        0   \n",
       "2             2            8      183             64              0        0   \n",
       "3             3            1       89             66             23       94   \n",
       "4             5            5      116             74              0        0   \n",
       "..          ...          ...      ...            ...            ...      ...   \n",
       "634         763           10      101             76             48      180   \n",
       "635         764            2      122             70             27        0   \n",
       "636         765            5      121             72             23      112   \n",
       "637         766            1      126             60              0        0   \n",
       "638         767            1       93             70             31        0   \n",
       "\n",
       "      BMI  DiabetesPedigreeFunction  Age  Outcome  \n",
       "0    33.6                     0.627   50        1  \n",
       "1    26.6                     0.351   31        0  \n",
       "2    23.3                     0.672   32        1  \n",
       "3    28.1                     0.167   21        0  \n",
       "4    25.6                     0.201   30        0  \n",
       "..    ...                       ...  ...      ...  \n",
       "634  32.9                     0.171   63        0  \n",
       "635  36.8                     0.340   27        0  \n",
       "636  26.2                     0.245   30        0  \n",
       "637  30.1                     0.349   47        1  \n",
       "638  30.4                     0.315   23        0  \n",
       "\n",
       "[639 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ FROM CSV AND CREATE A DATA FRAME\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Diabetes dataset/Clean_diabetes_dataset.csv')\n",
    "df\n",
    "\n",
    "# df_male = df.loc[df['gender'] == 'Male']    # GET ONLY THE MALE DATA(REASON: APATOTO FEMALE DATA THAK)\n",
    "\n",
    "# df = df.drop(\"gender\",axis=\"columns\")     # REMOVE GENDER COLUMN\n",
    "\n",
    "# df['gender'].replace('Male', 1, inplace=True)\n",
    "# df['gender'].replace('Female', 0, inplace=True)\n",
    "# df = df.loc[df['gender'] != 'Other']\n",
    "\n",
    "# df = df.drop(\"smoking_history\",axis=\"columns\")     # REMOVE SMOKING HISTORY COLUMN(REASON: DONT KNOW HOW SMOKING AFFECTS DIABETES)\n",
    "# df_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']\n",
    "# for i in col:\n",
    "#   df[i].replace(0,df[i].mean(),inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    439\n",
       "1    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT DF_MALE INTO X & Y\n",
    "# X = INDEPENDENT VARIABLES\n",
    "# Y = EPENDENT VARIABLES\n",
    "y = df['Outcome'].copy()\n",
    "X = df.drop(['Outcome','DiabetesPedigreeFunction'], axis=\"columns\")\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLE ADTA IMBALANCE - INCREASE/DECREASE ROWS WITH CLASS VALUES(0/1) IF ONE IS LOWER IN NUMBER\n",
    "# FOR BETTER TRAINING\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X_balanced, y_balanced = SMOTE().fit_resample(X, y)\n",
    "# y_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Scale X with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 32)                288       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2929 (11.44 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Dense(32, input_shape=(8,), activation='relu'))\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# model.add(keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=8, activation= 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary(\n",
    "    expand_nested=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 2s 5ms/step - loss: 0.6198 - accuracy: 0.6908\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.5050 - accuracy: 0.7495\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.4619 - accuracy: 0.7769\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7750\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.7769\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.7867\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4239 - accuracy: 0.7886\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4229 - accuracy: 0.7984\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4147 - accuracy: 0.7847\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.4095 - accuracy: 0.7906\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8004\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.7984\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3938 - accuracy: 0.7984\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8141\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8082\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8121\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.3763 - accuracy: 0.8082\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8258\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3687 - accuracy: 0.8297\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3606 - accuracy: 0.8141\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.8239\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3517 - accuracy: 0.8200\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8395\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3379 - accuracy: 0.8434\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3445 - accuracy: 0.8356\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.3305 - accuracy: 0.8434\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8493\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.8532\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8552\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.3114 - accuracy: 0.8552\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3021 - accuracy: 0.8630\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2986 - accuracy: 0.8689\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2984 - accuracy: 0.8748\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8591\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.2835 - accuracy: 0.8689\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2763 - accuracy: 0.8767\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8845\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2705 - accuracy: 0.8826\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2622 - accuracy: 0.8904\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2540 - accuracy: 0.8904\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.8865\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2429 - accuracy: 0.8982\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.8963\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.2285 - accuracy: 0.9139\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9002\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.2259 - accuracy: 0.8943\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9100\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9237\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9139\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.1938 - accuracy: 0.9354\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9022\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1938 - accuracy: 0.9217\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.9217\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1778 - accuracy: 0.9315\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.1745 - accuracy: 0.9432\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1589 - accuracy: 0.9452\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1577 - accuracy: 0.9413\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1544 - accuracy: 0.9413\n",
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.1499 - accuracy: 0.9511\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.1420 - accuracy: 0.9452\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1755 - accuracy: 0.9237\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.1375 - accuracy: 0.9589\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 1s 10ms/step - loss: 0.1355 - accuracy: 0.9569\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.1243 - accuracy: 0.9667\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.1189 - accuracy: 0.9667\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.1156 - accuracy: 0.9726\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.1145 - accuracy: 0.9628\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.1076 - accuracy: 0.9667\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1023 - accuracy: 0.9746\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0999 - accuracy: 0.9746\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0978 - accuracy: 0.9765\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9726\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9824\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0869 - accuracy: 0.9843\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0797 - accuracy: 0.9804\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0793 - accuracy: 0.9863\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9902\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 0.9902\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0703 - accuracy: 0.9883\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9883\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0630 - accuracy: 0.9961\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9883\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9941\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9922\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 0.9961\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0717 - accuracy: 0.9843\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0810 - accuracy: 0.9746\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0623 - accuracy: 0.9883\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0498 - accuracy: 0.9961\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9961\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0393 - accuracy: 1.0000\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.9961\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.9922\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.9941\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 0.9922\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.9961\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.9980\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 0.9980\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9941\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.9883\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 1s 11ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.9902\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9609\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.1254 - accuracy: 0.9569\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9863\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 0.9941\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=10,\n",
    "#     # callbacks=[earlystop_loss],\n",
    "# )\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 1.6752 - accuracy: 0.7422\n",
      "Accuracy: 74.22\n",
      "1.6752121448516846\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# print(\"Epochs run:\", len(history.history[\"loss\"]))\n",
    "\n",
    "# print(history.history.keys())\n",
    "# acc = history.history[\"accuracy\"]\n",
    "# loss = history.history[\"loss\"]\n",
    "\n",
    "# val_acc = history.history[\"val_accuracy\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# # Train and validation accuracy\n",
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.ylim((0, 1))\n",
    "# plt.plot(epochs, acc, label=\"Training accurarcy\")\n",
    "# plt.plot(epochs, val_acc, label=\"Validation accurarcy\")\n",
    "# plt.title(\"Training and Validation accurarcy\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Train and validation loss\n",
    "# plt.subplot(1, 2, 2)\n",
    "\n",
    "# plt.plot(epochs, loss, label=\"Training loss\")\n",
    "# plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
    "# plt.title(\"Training and Validation loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER TUNING\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# # defining parameter range\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf']} \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# # fitting the model for grid search\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # print best parameter after tuning\n",
    "# print(grid.best_params_)\n",
    "  \n",
    "# # print how our model looks after hyper-parameter tuning\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# PREDICT RESULTS\n",
    "\n",
    "# make probability predictions with the model\n",
    "y_pred = model.predict(X_test)\n",
    "# round predictions \n",
    "y_pred_rounded = [round(x[0]) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONFUSION MATRIX\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "cm = confusion_matrix(y_test, y_pred_rounded)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83        92\n",
      "           1       0.55      0.47      0.51        36\n",
      "\n",
      "    accuracy                           0.74       128\n",
      "   macro avg       0.68      0.66      0.67       128\n",
      "weighted avg       0.73      0.74      0.74       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae =>  0.2578125\n",
      "mse =>  0.2578125\n",
      "rmse =>  0.5077524002897476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred_rounded)\n",
    "mse = mean_squared_error(y_test, y_pred_rounded)\n",
    "rmse = np.sqrt(mse)\n",
    "print('mae => ', mae)\n",
    "print('mse => ', mse)\n",
    "print('rmse => ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL USING PICKLE PACKAGE\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # save the iris classification model as a pickle file\n",
    "# model_pkl_file = \"diabetes-model-ann.pkl\"  \n",
    "\n",
    "# with open(model_pkl_file, 'wb') as file:  \n",
    "#     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOAD AND USE THE SAVED MODEL USING PICKLE PACKAGE\n",
    "# with open(model_pkl_file, 'rb') as file:  \n",
    "#     model = pickle.load(file)\n",
    "\n",
    "# # evaluate model \n",
    "# y_predict = model.predict(X_test)\n",
    "\n",
    "# # check results\n",
    "# pred = model.evaluate(X_test, y_test)\n",
    "# print(f\"Accuracy : {pred * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
