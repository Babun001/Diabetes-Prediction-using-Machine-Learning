{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>763</td>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>764</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>765</td>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>766</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>767</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>639 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  \\\n",
       "0             0            6      148             72             35        0   \n",
       "1             1            1       85             66             29        0   \n",
       "2             2            8      183             64              0        0   \n",
       "3             3            1       89             66             23       94   \n",
       "4             5            5      116             74              0        0   \n",
       "..          ...          ...      ...            ...            ...      ...   \n",
       "634         763           10      101             76             48      180   \n",
       "635         764            2      122             70             27        0   \n",
       "636         765            5      121             72             23      112   \n",
       "637         766            1      126             60              0        0   \n",
       "638         767            1       93             70             31        0   \n",
       "\n",
       "      BMI  DiabetesPedigreeFunction  Age  Outcome  \n",
       "0    33.6                     0.627   50        1  \n",
       "1    26.6                     0.351   31        0  \n",
       "2    23.3                     0.672   32        1  \n",
       "3    28.1                     0.167   21        0  \n",
       "4    25.6                     0.201   30        0  \n",
       "..    ...                       ...  ...      ...  \n",
       "634  32.9                     0.171   63        0  \n",
       "635  36.8                     0.340   27        0  \n",
       "636  26.2                     0.245   30        0  \n",
       "637  30.1                     0.349   47        1  \n",
       "638  30.4                     0.315   23        0  \n",
       "\n",
       "[639 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# READ FROM CSV AND CREATE A DATA FRAME\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./Diabetes dataset/Clean_diabetes_dataset.csv')\n",
    "df\n",
    "\n",
    "# df_male = df.loc[df['gender'] == 'Male']    # GET ONLY THE MALE DATA(REASON: APATOTO FEMALE DATA THAK)\n",
    "\n",
    "# df = df.drop(\"gender\",axis=\"columns\")     # REMOVE GENDER COLUMN\n",
    "\n",
    "# df['gender'].replace('Male', 1, inplace=True)\n",
    "# df['gender'].replace('Female', 0, inplace=True)\n",
    "# df = df.loc[df['gender'] != 'Other']\n",
    "\n",
    "# df = df.drop(\"smoking_history\",axis=\"columns\")     # REMOVE SMOKING HISTORY COLUMN(REASON: DONT KNOW HOW SMOKING AFFECTS DIABETES)\n",
    "# df_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col=['Glucose' ,'BloodPressure' ,'SkinThickness', 'Insulin' ,'BMI']\n",
    "# for i in col:\n",
    "#   df[i].replace(0,df[i].mean(),inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outcome\n",
       "0    439\n",
       "1    200\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT DF_MALE INTO X & Y\n",
    "# X = INDEPENDENT VARIABLES\n",
    "# Y = EPENDENT VARIABLES\n",
    "y = df['Outcome'].copy()\n",
    "X = df.drop(['Outcome','DiabetesPedigreeFunction'], axis=\"columns\")\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDLE ADTA IMBALANCE - INCREASE/DECREASE ROWS WITH CLASS VALUES(0/1) IF ONE IS LOWER IN NUMBER\n",
    "# FOR BETTER TRAINING\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# X_balanced, y_balanced = SMOTE().fit_resample(X, y)\n",
    "# y_balanced.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN-TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "# Scale X with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                288       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 520       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2929 (11.44 KB)\n",
      "Trainable params: 2929 (11.44 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Dense(32, input_shape=(8,), activation='relu'))\n",
    "# model.add(keras.layers.Dense(64, activation='relu'))\n",
    "# model.add(keras.layers.Dense(16, activation='relu'))\n",
    "# model.add(keras.layers.Dense(8, activation='relu'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=8, activation= 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.summary(\n",
    "    expand_nested=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "52/52 [==============================] - 1s 2ms/step - loss: 0.6027 - accuracy: 0.6967\n",
      "Epoch 2/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4927 - accuracy: 0.7808\n",
      "Epoch 3/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7710\n",
      "Epoch 4/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7710\n",
      "Epoch 5/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4303 - accuracy: 0.7828\n",
      "Epoch 6/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.7828\n",
      "Epoch 7/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.7965\n",
      "Epoch 8/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.7945\n",
      "Epoch 9/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.4049 - accuracy: 0.8004\n",
      "Epoch 10/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8141\n",
      "Epoch 11/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8082\n",
      "Epoch 12/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8063\n",
      "Epoch 13/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3934 - accuracy: 0.8200\n",
      "Epoch 14/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8141\n",
      "Epoch 15/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8317\n",
      "Epoch 16/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8160\n",
      "Epoch 17/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.3691 - accuracy: 0.8258\n",
      "Epoch 18/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3620 - accuracy: 0.8239\n",
      "Epoch 19/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8376\n",
      "Epoch 20/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8337\n",
      "Epoch 21/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8356\n",
      "Epoch 22/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.8395\n",
      "Epoch 23/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8454\n",
      "Epoch 24/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8454\n",
      "Epoch 25/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8493\n",
      "Epoch 26/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8474\n",
      "Epoch 27/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8532\n",
      "Epoch 28/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3183 - accuracy: 0.8630\n",
      "Epoch 29/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8650\n",
      "Epoch 30/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3057 - accuracy: 0.8748\n",
      "Epoch 31/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8650\n",
      "Epoch 32/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2953 - accuracy: 0.8806\n",
      "Epoch 33/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.8728\n",
      "Epoch 34/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.8787\n",
      "Epoch 35/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2753 - accuracy: 0.8865\n",
      "Epoch 36/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8826\n",
      "Epoch 37/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2669 - accuracy: 0.8904\n",
      "Epoch 38/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8845\n",
      "Epoch 39/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.8963\n",
      "Epoch 40/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2734 - accuracy: 0.8943\n",
      "Epoch 41/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.8669\n",
      "Epoch 42/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.8982\n",
      "Epoch 43/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.2423 - accuracy: 0.9041\n",
      "Epoch 44/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.9002\n",
      "Epoch 45/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8982\n",
      "Epoch 46/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9100\n",
      "Epoch 47/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.9237\n",
      "Epoch 48/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9178\n",
      "Epoch 49/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9119\n",
      "Epoch 50/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9217\n",
      "Epoch 51/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1951 - accuracy: 0.9276\n",
      "Epoch 52/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.9237\n",
      "Epoch 53/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9335\n",
      "Epoch 54/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1811 - accuracy: 0.9315\n",
      "Epoch 55/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1780 - accuracy: 0.9374\n",
      "Epoch 56/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1736 - accuracy: 0.9472\n",
      "Epoch 57/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.9472\n",
      "Epoch 58/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.9393\n",
      "Epoch 59/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1637 - accuracy: 0.9491\n",
      "Epoch 60/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9432\n",
      "Epoch 61/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1490 - accuracy: 0.9589\n",
      "Epoch 62/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1450 - accuracy: 0.9530\n",
      "Epoch 63/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1488 - accuracy: 0.9452\n",
      "Epoch 64/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.9628\n",
      "Epoch 65/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1338 - accuracy: 0.9648\n",
      "Epoch 66/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1277 - accuracy: 0.9648\n",
      "Epoch 67/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1544 - accuracy: 0.9472\n",
      "Epoch 68/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1362 - accuracy: 0.9530\n",
      "Epoch 69/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9667\n",
      "Epoch 70/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9687\n",
      "Epoch 71/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9706\n",
      "Epoch 72/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9804\n",
      "Epoch 73/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.1025 - accuracy: 0.9785\n",
      "Epoch 74/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0977 - accuracy: 0.9804\n",
      "Epoch 75/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9824\n",
      "Epoch 76/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9804\n",
      "Epoch 77/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0875 - accuracy: 0.9824\n",
      "Epoch 78/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9863\n",
      "Epoch 79/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9824\n",
      "Epoch 80/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0878 - accuracy: 0.9804\n",
      "Epoch 81/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9863\n",
      "Epoch 82/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0698 - accuracy: 0.9883\n",
      "Epoch 83/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9883\n",
      "Epoch 84/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0644 - accuracy: 0.9922\n",
      "Epoch 85/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0731 - accuracy: 0.9902\n",
      "Epoch 86/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0618 - accuracy: 0.9902\n",
      "Epoch 87/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9961\n",
      "Epoch 88/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9941\n",
      "Epoch 89/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0501 - accuracy: 0.9922\n",
      "Epoch 90/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9941\n",
      "Epoch 91/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9941\n",
      "Epoch 92/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9961\n",
      "Epoch 93/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0420 - accuracy: 0.9961\n",
      "Epoch 94/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9961\n",
      "Epoch 95/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 0.9961\n",
      "Epoch 96/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9961\n",
      "Epoch 97/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.9941\n",
      "Epoch 98/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9961\n",
      "Epoch 99/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9980\n",
      "Epoch 100/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9961\n",
      "Epoch 101/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.9980\n",
      "Epoch 102/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9980\n",
      "Epoch 103/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9980\n",
      "Epoch 104/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9980\n",
      "Epoch 105/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9941\n",
      "Epoch 106/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 0.9980\n",
      "Epoch 107/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9980\n",
      "Epoch 108/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9980\n",
      "Epoch 109/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9980\n",
      "Epoch 110/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9961\n",
      "Epoch 112/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9980\n",
      "Epoch 113/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 118/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 119/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 120/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 121/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.9961\n",
      "Epoch 122/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.9980\n",
      "Epoch 123/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 125/150\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 126/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 131/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 132/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 133/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 134/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 135/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "52/52 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 140/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 141/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 142/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 143/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 144/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 149/150\n",
      "52/52 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 150/150\n",
      "52/52 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# earlystop_loss = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     validation_split=0.2,\n",
    "#     epochs=50,\n",
    "#     batch_size=10,\n",
    "#     # callbacks=[earlystop_loss],\n",
    "# )\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 1.6704 - accuracy: 0.6875\n",
      "Accuracy: 68.75\n",
      "1.6704144477844238\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# print(\"Epochs run:\", len(history.history[\"loss\"]))\n",
    "\n",
    "# print(history.history.keys())\n",
    "# acc = history.history[\"accuracy\"]\n",
    "# loss = history.history[\"loss\"]\n",
    "\n",
    "# val_acc = history.history[\"val_accuracy\"]\n",
    "# val_loss = history.history[\"val_loss\"]\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# # Train and validation accuracy\n",
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.ylim((0, 1))\n",
    "# plt.plot(epochs, acc, label=\"Training accurarcy\")\n",
    "# plt.plot(epochs, val_acc, label=\"Validation accurarcy\")\n",
    "# plt.title(\"Training and Validation accurarcy\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Train and validation loss\n",
    "# plt.subplot(1, 2, 2)\n",
    "\n",
    "# plt.plot(epochs, loss, label=\"Training loss\")\n",
    "# plt.plot(epochs, val_loss, label=\"Validation loss\")\n",
    "# plt.title(\"Training and Validation loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER PARAMETER TUNING\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "# # defining parameter range\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf']} \n",
    "  \n",
    "# grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# # fitting the model for grid search\n",
    "# grid.fit(X_train, y_train)\n",
    "\n",
    "# # print best parameter after tuning\n",
    "# print(grid.best_params_)\n",
    "  \n",
    "# # print how our model looks after hyper-parameter tuning\n",
    "# print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# PREDICT RESULTS\n",
    "\n",
    "# make probability predictions with the model\n",
    "y_pred = model.predict(X_test)\n",
    "# round predictions \n",
    "y_pred_rounded = [round(x[0]) for x in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CONFUSION MATRIX\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "cm = confusion_matrix(y_test, y_pred_rounded)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77        92\n",
      "           1       0.45      0.56      0.50        36\n",
      "\n",
      "    accuracy                           0.69       128\n",
      "   macro avg       0.63      0.65      0.64       128\n",
      "weighted avg       0.71      0.69      0.70       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL USING PICKLE PACKAGE\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # save the iris classification model as a pickle file\n",
    "# model_pkl_file = \"diabetes-model-ann.pkl\"  \n",
    "\n",
    "# with open(model_pkl_file, 'wb') as file:  \n",
    "#     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LOAD AND USE THE SAVED MODEL USING PICKLE PACKAGE\n",
    "# with open(model_pkl_file, 'rb') as file:  \n",
    "#     model = pickle.load(file)\n",
    "\n",
    "# # evaluate model \n",
    "# y_predict = model.predict(X_test)\n",
    "\n",
    "# # check results\n",
    "# pred = model.evaluate(X_test, y_test)\n",
    "# print(f\"Accuracy : {pred * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
